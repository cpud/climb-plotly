{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "#Trying to get hyperlinks for all states so we can run the scraping script on all states.\n",
    "\n",
    "#Imports and get the html data from MountainProject. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.mountainproject.com/route-guide'\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#Put all hyperlinks on the page into a list.\n",
    "data = []\n",
    "for link in soup.find_all('a', class_= 'text-truncate float-xs-left'):\n",
    "    data.append(link.get('href'))\n",
    "    \n",
    "#List of US States that we'll use to remove unwanted hyperlinks.\n",
    "#Mississippi and Nebrasaka don't have pages, which makes sense.\n",
    "states = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New-Hampshire\",\"New-Jersey\",\"New-Mexico\",\"New-York\",\n",
    "  \"North-Carolina\",\"North-Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode-Island\",\"South-Carolina\",\"South-Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West-Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "\n",
    "#List of US States and their abbreviations for a dictionary.\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New-Hampshire': 'NH',\n",
    "    'New-Jersey': 'NJ',\n",
    "    'New-Mexico': 'NM',\n",
    "    'New-York': 'NY',\n",
    "    'North-Carolina': 'NC',\n",
    "    'North-Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Palau': 'PW',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode-Island': 'RI',\n",
    "    'South-Carolina': 'SC',\n",
    "    'South-Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West-Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}\n",
    "\n",
    "#Make the states lowercase to match hyperlinks. \n",
    "states = [x.lower() for x in states]\n",
    "us_state_abbrev = dict((k.lower(),v) for k,v in us_state_abbrev.items())\n",
    "\n",
    "#Loop through each hyperlink and see if it contains a state as a subsrting.\n",
    "#If it does, add the link to a list containing links for each state page. \n",
    "US = []\n",
    "for datum in data:\n",
    "    for state in states:\n",
    "        if state in datum:\n",
    "            US.append([state,datum])\n",
    "        \n",
    "#Remove duplicate links, Vermont Ice+Mixed, and Tennessee Wall (they are duplicates!)\n",
    "#Check length. Should be 48. \n",
    "res = [] \n",
    "[res.append(x) for x in US if x not in res] \n",
    "#US=list(set(US))\n",
    "res.remove(['tennessee','https://www.mountainproject.com/area/105851828/the-tennessee-wall'])\n",
    "res.remove(['vermont','https://www.mountainproject.com/area/107280521/vermont-ice-and-mixed'])\n",
    "links = dict(res)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to get number of crags on an individual state page.\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.mountainproject.com/area/105708959/california'\n",
    "\n",
    "def getData(url):\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Getting the webpage, creating a Response object.\n",
    "    response = requests.get(url)\n",
    " \n",
    "    # Extracting the source code of the page.\n",
    "    data = response.text\n",
    " \n",
    "    # Passing the source code to Beautiful Soup to create a BeautifulSoup object for it.\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    " \n",
    "    # Extracting all the <a> tags into a list. No ID or class or anything next to hyperlink in HTML\n",
    "    titles = soup.findAll('a')\n",
    " \n",
    "    # Extracting text from the the <a> and put into list. \n",
    "    final = []\n",
    "    for title in titles:\n",
    "        final.append(title.text)\n",
    "            \n",
    "    df = pd.DataFrame(final)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Define function to get location of a string that always occurs after end of list of crags.\n",
    "def getIndexes(dfObj, value):\n",
    "    ''' Get index positions of value in dataframe i.e. dfObj.'''\n",
    " \n",
    "    listOfPos = list()\n",
    "    # Get bool dataframe with True at positions where the given value exists\n",
    "    result = dfObj.isin([value])\n",
    "    # Get list of columns that contains the value\n",
    "    seriesObj = result.any()\n",
    "    columnNames = list(seriesObj[seriesObj == True].index)\n",
    "    # Iterate over list of columns and fetch the rows indexes where value exists\n",
    "    for col in columnNames:\n",
    "        rows = list(result[col][result[col] == True].index)\n",
    "        for row in rows:\n",
    "            listOfPos.append((row, col))\n",
    "    # Return a list of tuples indicating the positions of value in the dataframe\n",
    "    return listOfPos\n",
    "\n",
    "#Define function to remove the unwanted text from the dataframe. Pages with slight differences\n",
    "#in their layout needed the cutOff point to be manually entered.\n",
    "def adjustDF(url,state):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df = getData(url)\n",
    "\n",
    "    #Find location of desired string (the first string after last crag), remove everything after it. \n",
    "    if 'delaware' in url:\n",
    "        cutOff = [(48,0)]\n",
    "    elif 'illinois' in url:\n",
    "        cutOff = [(74,0)]\n",
    "    elif 'kentucky' in url:\n",
    "        cutOff = [(56,0)]\n",
    "    elif 'louisiana' in url:\n",
    "        cutOff = [(47,0)]\n",
    "    elif 'new-jersey' in url:\n",
    "        cutOff = [(51,0)]\n",
    "    elif 'rhode-island' in url:\n",
    "        cutOff = [(67,0)]\n",
    "    else:\n",
    "        cutOff = getIndexes(df,'\\nPrevious\\n')\n",
    "        \n",
    "    df.drop(df.index[cutOff[0][0]:(len(df))],inplace = True)\n",
    "\n",
    "\n",
    "    #Remove text displayed before climbing spots.\n",
    "    if 'new-jersey' in url:\n",
    "        df.drop(df.index[0:45], inplace = True)\n",
    "    else:\n",
    "        df.drop(df.index[0:46], inplace = True)\n",
    "\n",
    "    #Rename column to 'crags'\n",
    "    df.columns = ['Crags']\n",
    "\n",
    "    #Add state column\n",
    "    df = df.assign(ST = us_state_abbrev[state])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make combined dataframe for all crags in all states using gathered hyperlinks.\n",
    "def makeDataFrame(links):\n",
    "    #Instantiate empty dataframe\n",
    "    hope = pd.DataFrame()\n",
    "    #Loop through links.\n",
    "    for sta, link in links.items():\n",
    "        #Add state dataframes on top of each other.\n",
    "        hope = hope.append(adjustDF(link,sta))\n",
    "    return hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1167 entries, 46 to 77\n",
      "Data columns (total 2 columns):\n",
      "Crags    1167 non-null object\n",
      "ST       1167 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 27.4+ KB\n"
     ]
    }
   ],
   "source": [
    "all_df = makeDataFrame(links)\n",
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
